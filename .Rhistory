install.packages("pdflatex")
Sys.which("pdflatex")
install.packages("truncnorm")
library(PolyGibbs)
?BinaryGibbs_fit
library(PolyGibbs)
?BinaryGibbs_fit
library(PolyGibbs)
?BinaryGibbs_fit
library(PolyGibbs)
?BinaryGibbs_fit
install.packages("truncnorm")
library(truncnorm)
library(PolyGibbs)
?BinaryGibbs_fit
set.seed(250)
require(truncnorm)
require(MASS)
N <- 500
x1 <- seq(-1, 1, length.out = N)
x2 <- rnorm(N, 0, 1)
D <- 3
X <- matrix(c(rep(1, N), x1, x2), ncol = D)
true_theta <- c(-.5, 3.3, 2)
p <- pnorm(X %*% true_theta)
y <- rbinom(N, 1, p)
N1  <- sum(y)  # Number of successes
N0  <- N - N1  # Number of failures
#Spliting The Data in Train and Test in 80:20 ratio
Train_ID = sample(1:nrow(X), round(nrow(X) * 0.8), replace = FALSE) # Train Data IDS
Train_X = X[Train_ID, -1] # Train Data Covariates
Test_X = X[-Train_ID, -1] # Test Data Covarites
Train_Y = y[Train_ID] # Train Data Response
Test_Y = y[-Train_ID] # Test Data Response
nIter = 10000
burn_in = round(nIter * 0.5)
prior = 2
prior_mean = rep(1, 3)
prior_var = diag(10, 3)
BinaryGibbs_fit(Train_X, Train_Y, nIter, prior, burn_in, prior_mean, prior_var )
library(PolyGibbs)
?BinaryGibbs_fit
set.seed(250)
require(truncnorm)
require(MASS)
N <- 500
x1 <- seq(-1, 1, length.out = N)
x2 <- rnorm(N, 0, 1)
D <- 3
X <- matrix(c(rep(1, N), x1, x2), ncol = D)
true_theta <- c(- 1, 3, 2)
p <- pnorm(X %*% true_theta)
y <- rbinom(N, 1, p)
N1  <- sum(y)  # Number of successes
N0  <- N - N1  # Number of failures
#Spliting The Data in Train and Test in 80:20 ratio
Train_ID = sample(1:nrow(X), round(nrow(X) * 0.8), replace = FALSE) # Train Data IDS
Train_X = X[Train_ID, -1] # Train Data Covariates
Test_X = X[-Train_ID, -1] # Test Data Covarites
Train_Y = y[Train_ID] # Train Data Response
Test_Y = y[-Train_ID] # Test Data Response
nIter = 10000
burn_in = round(nIter * 0.5)
prior = 2
prior_mean = rep(1, 3)
prior_var = diag(10, 3)
BinaryGibbs_fit(Train_X, Train_Y, nIter, prior, burn_in, prior_mean, prior_var )
library(PolyGibbs)
?BinaryGibbs_fit
set.seed(250)
require(truncnorm)
require(MASS)
N <- 500
x1 <- seq(-1, 1, length.out = N)
x2 <- rnorm(N, 0, 1)
D <- 3
X <- matrix(c(rep(1, N), x1, x2), ncol = D)
true_theta <- c(- 1.2, 3.5, 2.4)
p <- pnorm(X %*% true_theta)
y <- rbinom(N, 1, p)
N1  <- sum(y)  # Number of successes
N0  <- N - N1  # Number of failures
#Spliting The Data in Train and Test in 80:20 ratio
Train_ID = sample(1:nrow(X), round(nrow(X) * 0.8), replace = FALSE) # Train Data IDS
Train_X = X[Train_ID, -1] # Train Data Covariates
Test_X = X[-Train_ID, -1] # Test Data Covarites
Train_Y = y[Train_ID] # Train Data Response
Test_Y = y[-Train_ID] # Test Data Response
nIter = 10000
burn_in = round(nIter * 0.5)
prior = 2
prior_mean = rep(1, 3)
prior_var = diag(10, 3)
BinaryGibbs_fit(Train_X, Train_Y, nIter, prior, burn_in, prior_mean, prior_var )
set.seed(250)
require(truncnorm)
require(MASS)
N <- 500
x1 <- seq(-1, 1, length.out = N)
x2 <- rnorm(N, 0, 1)
D <- 3
X <- matrix(c(rep(1, N), x1, x2), ncol = D)
true_theta <- c(- 1.2, 3.5, 2.4)
p <- pnorm(X %*% true_theta)
y <- rbinom(N, 1, p)
N1  <- sum(y)  # Number of successes
N0  <- N - N1  # Number of failures
#Spliting The Data in Train and Test in 80:20 ratio
Train_ID = sample(1:nrow(X), round(nrow(X) * 0.8), replace = FALSE) # Train Data IDS
Train_X = X[Train_ID, -1] # Train Data Covariates
Test_X = X[-Train_ID, -1] # Test Data Covarites
Train_Y = y[Train_ID] # Train Data Response
Test_Y = y[-Train_ID] # Test Data Response
nIter = 10000
burn_in = round(nIter * 0.5)
prior = 1
prior_mean = rep(1, 3)
prior_var = diag(10, 3)
BinaryGibbs_fit(Train_X, Train_Y, nIter, prior, burn_in, prior_mean, prior_var )
# Data Generation------------------------------------------------------------------
set.seed(250)
N <- 500
x1 <- seq(-1, 1, length.out = N)
x2 <- rnorm(N, 0, 1)
# Create n x D design matrix
D <- 3
# We learn a linear function
X <- matrix(c(rep(1, N), x1, x2), ncol = D)
# True values of regression coeffiecients theta
true_theta <- c(-.5, 3.3, 2)
# Obtain the vector with probabilities of success p using the probit link
p <- pnorm(X %*% true_theta)
# Generate binary observation data y
y <- rbinom(N, 1, p)
# Variables that we will need later
N1  <- sum(y)  # Number of successes
N0  <- N - N1  # Number of failures
#Spliting The Data in Train and Test in 80:20 ratio
Train_ID = sample(1:nrow(X), round(nrow(X) * 0.8), replace = FALSE) # Train Data IDS
Train_X = X[Train_ID, -1] # Train Data Covariates
Test_X = X[-Train_ID, -1] # Test Data Covarites
Train_Y = y[Train_ID] # Train Data Response
Test_Y = y[-Train_ID] # Test Data Response
nIter = 10000
burn_in = round(nIter * 0.5)
prior = 2
prior_mean = rep(1, 3)
prior_var = diag(10, 3)
# Model Fitting -----------------------------------------------------------
#Libraries required
#install.packages("MASS")
#install.packages("truncnorm")
library(MASS)
library(PolyGibbs)
library(PolyGibbs)
? BinaryGibbs_fit
set.seed(250)
require(truncnorm)
require(MASS)
N <- 500
x1 <- seq(-1, 1, length.out = N)
x2 <- rnorm(N, 0, 1)
D <- 3
X <- matrix(c(rep(1, N), x1, x2), ncol = D)
true_theta <- c(- 0.5, 3.3, 2)
p <- pnorm(X %*% true_theta)
y <- rbinom(N, 1, p)
N1  <- sum(y)  # Number of successes
N0  <- N - N1  # Number of failures
#Spliting The Data in Train and Test in 80:20 ratio
Train_ID = sample(1:nrow(X), round(nrow(X) * 0.8), replace = FALSE) # Train Data IDS
Train_X = X[Train_ID, -1] # Train Data Covariates
Test_X = X[-Train_ID, -1] # Test Data Covarites
Train_Y = y[Train_ID] # Train Data Response
Test_Y = y[-Train_ID] # Test Data Response
nIter = 10000
burn_in = round(nIter * 0.5)
prior = 2
prior_mean = rep(1, 3)
prior_var = diag(10, 3)
BinaryGibbs_fit(Train_X, Train_Y, nIter, prior, burn_in, prior_mean, prior_var )
?BinaryGibbs_fit
getAnywhere(BinaryGibbs_fit)
library(PolyGibbs)
?BinaryGibbs_fit
set.seed(250)
require(truncnorm)
require(MASS)
N <- 500
x1 <- seq(-1, 1, length.out = N)
x2 <- rnorm(N, 0, 1)
D <- 3
X <- matrix(c(rep(1, N), x1, x2), ncol = D)
true_theta <- c(- 0.5, 3.3, 2)
p <- pnorm(X %*% true_theta)
y <- rbinom(N, 1, p)
N1  <- sum(y)  # Number of successes
N0  <- N - N1  # Number of failures
#Spliting The Data in Train and Test in 80:20 ratio
Train_ID = sample(1:nrow(X), round(nrow(X) * 0.8), replace = FALSE) # Train Data IDS
Train_X = X[Train_ID, -1] # Train Data Covariates
Test_X = X[-Train_ID, -1] # Test Data Covarites
Train_Y = y[Train_ID] # Train Data Response
Test_Y = y[-Train_ID] # Test Data Response
nIter = 10000
burn_in = round(nIter * 0.5)
prior = 2
prior_mean = rep(1, 3)
prior_var = diag(10, 3)
BinaryGibbs_fit(Train_X, Train_Y, nIter, prior, burn_in, prior_mean, prior_var )
?BinaryGibbs_Pred
Output = BinaryGibbs_fit(Train_X, Train_Y, nIter, prior, burn_in, prior_mean, prior_var )
estimates = Output$estimates
BinaryGibbs_Pred(estimates, Test_X)
?BinaryGibbs_Test_Accuracy
Output = BinaryGibbs_fit(Train_X, Train_Y, nIter, prior, burn_in, prior_mean, prior_var )
estimates = Output$estimates
Predicted_Y = BinaryGibbs_Pred(estimates, Test_X)
BinaryGibbs_Test_Accuracy(Predicted_Y, Test_Y)
?BinaryGibbs_Traceplot
temp = BinaryGibbs_fit(Train_X, Train_Y, nIter, prior, burn_in, prior_mean, prior_var )
BinaryGibbs_Traceplot(beta_matrix = temp$beta_matrix, k = 0)
?BinaryGibbs_PosteriorDistribution_plot
temp = BinaryGibbs_fit(Train_X, Train_Y, nIter, prior, burn_in, prior_mean, prior_var )
BinaryGibbs_PosteriorDistribution_plot(beta_matrix = temp$beta_matrix , k = 0, burn_in = 2500, breaks= 50)
?MultinomGibbs_fit
# Initialization
set.seed(250)
n <- 1000 # Total no of observations.
int1 <- -1 # gamma boundary
int2 <- 3  # gamma boundary
beta <- c(-.75, 1) # Regression Parameters for data generation.
X <- cbind(sample(1:4, n, replace = TRUE), rnorm(n, 0, 2)) # Generated design matrix
# Generation of Latent Variable Observations
eta <- X %*% beta
z <- rnorm(n, eta, 1)
# Generation of Responses depending on z
y <- rep(0, n)
y[z <= int1] <- 1
y[int1 <z & z <= int2] <- 2
y[int2 < z ] <- 3
#Spliting The Data in Train and Test in 80:20 ratio
Train_ID = sample(1:nrow(X), round(nrow(X) * 0.8), replace = FALSE) # Train Data IDS
Train_X = X[Train_ID, ]# Train Data Covariates
Test_X = X[-Train_ID, ]
Train_Y = y[Train_ID] # Train Data Response
Test_Y = y[-Train_ID] # Test Data Response
K = 3
nIter = 10000
burn_in = 5000
MultinomGibbs_fit(Train_X, Train_Y, nIter, burn_in, K)
?MultinomGibbs_pred
Result = MultinomGibbs_fit(Train_X, Train_Y, nIter, burn_in, K)
estimates = Result$estimates
gamma_estimates = Result$gamma_estimates
MultinomGibbs_pred(estimates, gamma_estimates,Test_X )
?MultinomGibbs_Test_Accuracy
Result = MultinomGibbs_fit(Train_X, Train_Y, nIter, burn_in, K)
estimates = Result$estimates
gamma_estimates = Result$gamma_estimates
Result_Pred = MultinomGibbs_pred(estimates, gamma_estimates,Test_X )
Predicted_Y = Result_Pred
MultinomGibbs_Test_Accuracy(Predicted_Y, Test_Y, K)
?Multinom_traceplot_beta
?BinaryGibbs_Traceplot
?BinaryGibbs_PosteriorDistribution_plot
?Multinom_traceplot_beta
Result = MultinomGibbs_fit(Train_X, Train_Y, nIter, burn_in, K)
beta_matrix = Result$beta_matrix
Multinom_traceplot_beta(beta_matrix = beta_matrix, k = 1)
?Multinom_PosteriorDistribution_plot_gamma
Result = MultinomGibbs_fit(Train_X, Train_Y, nIter, burn_in, K)
beta_matrix = Result$beta_matrix
Multinom_traceplot_beta(beta_matrix = beta_matrix, k = 1)
?Multinom_PosteriorDistribution_plot_beta
Result = MultinomGibbs_fit(Train_X, Train_Y, nIter, burn_in, K)
beta_matrix = Result$beta_matrix
Multinom_PosteriorDistribution_plot_beta(beta_matrix = beta_matrix , k = 2, burn_in = 2500, breaks= 50)
?Multinom_PosteriorDistribution_plot_gamma
Result = MultinomGibbs_fit(Train_X, Train_Y, nIter, burn_in, K)
gamma_update = Result$gamma_update
Multinom_PosteriorDistribution_plot_gamma(gamma_update = gamma_update , k = 2, burn_in = 2500, breaks= 50)
